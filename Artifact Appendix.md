# Artifact Appendix

Paper title: **Evaluating Google's Protected Audience Protocol**

Artifacts HotCRP Id: **#80**

Requested Badge: **Reproducible**

## Description
The code for producing our simulation results and corresponding graphs (i.e. Figure 2, Table 1, and Figure 3).

## Basic Requirements

### Estimated Time and Storage Consumption
Using the provided data without generating your own simulation data, the estimated time should be a few minutes. 

## Environment
In the following, describe how to access our artifact and all related and necessary data and software components.
Afterward, describe how to set up everything and how to verify that everything is set up correctly.

### Accessibility
The code can be accessed at github at https://github.com/Elena6918/PrAu-Simulation.


### Set up the environment
Describe how the reviews should set up the environment for your artifacts, including download and install dependencies and the installation of the artifact itself.
Be as specific as possible here.
If possible, use code segments to simply the workflow, e.g.,

```bash
git clone https://github.com/Elena6918/PrAu-Simulation
pip install numpy
pip install scipy 
pip install matplotlib
pip install mmh3
pip install bitarray
```

## Artifact Evaluation
This section includes all the steps required to evaluate your artifact's functionality and validate your paper's key results and claims.
Therefore, highlight your paper's main results and claims in the first subsection. And describe the experiments that support your claims in the subsection after that.

### Main Results and Claims
List all your paper's main results and claims that are supported by your submitted artifacts.

#### Main Result 1: Figure 2
A figure that shows the relationship between the number of colluding buyers and the expected accuracy of the adversary’s prediction algorithm in scenario2. An integral evaluation by computation software. 

#### Main Result 2: Figure 3
A figure that shows the relationship between the selection of the 
number of accusations and the False Positive Rate (FPR) achieved
by the adversary’s prediction algorithm when the adversary controls 20 buyers for different size candidate pools. 

#### Main Result 3: Table 1
A table that shows the smallest number of colluding buyers the
adversary needs to control to achieve a PPV over 0.99 with respect to different privacy loss budget $\epsilon$ and the number of accusations.

### Experiments
List each experiment the reviewer has to execute. Describe:
 - How to execute it in detailed steps.
 - What the expected result is.
 - How long it takes and how much space it consumes on disk. (approximately)
 - Which claim and results does it support, and how.

#### Experiment 1: Figure 2
```bash
python scenario_2_accuracy.py
```
The expected result is a pdf named "Accuracy", which match the Figure 2 of the paper. Should take less than a minute to run. 
#### Experiment 2: Figure 3
```bash
python plot_fpr.py
```
The expected result is a pdf named "FPR_n20_top_k_extended.pdf", which match the Figure 3 of the paper. Should take a few minutes to run. 

#### Experiment 3: Table
```bash
python simulate_colluder_num.py
```
The expected results should be a few lines printed to the console, showing the mean and variance of five simulations. Should take less than a minute to run. 

## Limitations
The code above only reproduce the graphs based on our provided simulation result data. The folder ``num_colluders`` contains the result of five simulations to obtain ``Number of Accusations``. This data is generated by repeatedly running the ``smallest_number()`` function in ``simulate_colluder_num.py`` with adjusted parameters. Similarly, data to generate 3 is provided in folder ``metrics_e1`` and ``metrics_e10``, where the dictionary key is the pool size, the first number represents the number of colluders, and the list contains the number of [true positives, true negatives, false positives, false negatives]. These metrics data is generated by running ``simulation_metrics.py`` repeatedly with different parameters. Since the simulation result includes randomness, every time the simulation code is ran, the result should be slightly different. Therefore, we record our simulation result and provide the code to generate graphs based on them, as they are the exact ones we present in the paper. 



